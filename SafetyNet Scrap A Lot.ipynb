{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KnapsackNetsModified import SafetyNet\n",
    "#from KnapsackNets import SafetyNet\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torch.nn import Parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Simulation Parameters\n",
    "\n",
    "From JK thesis!\n",
    "- https://www.cmu.edu/tepper/programs/phd/program/assets/dissertations/2017-joint-phd-program-algorithms-combinatorics-and-optimization-karp-dissertation.pdf\n",
    "\n",
    "```\n",
    "Note that at larger cluster numbers, the improved performance of our\n",
    "knapsack models will in fact be significantly more pronounced but even as few as\n",
    "50 clusters is enough to make a significant impact. Demand for each cluster is mod-\n",
    "eled as a Poisson process whose rate parameter is drawn from a Normal distribution\n",
    "with mean 10 and standard deviation 5. Each day of the simulation allows the Pois-\n",
    "son processes generating demand to run for either 40 or 400 time units (depending\n",
    "on simulation condition). Inventory distributions for each category are modeled as\n",
    "Poisson distributions with rate parameters drawn from a Uniform distribution from\n",
    "1 to 20. The price of items in each category is drawn from a Normal distribution\n",
    "with mean 800 and standard deviation 400. Each order is assigned a cancel probabil-\n",
    "ity based on a logistic function of the form 1 − 1/1+ea+b·I , where I is the item’s stated \n",
    "inventory level and a and b are parameters of the item’s category. a and b are drawn for each category from Normal distribution with mean .5 and −.2 and standard deviations .1 and either .05 or .15 (depending on simulation condition), respectively. The inventory distribution parameters, cancel parameters, demand rates, and prices are all truncated at 0 to prevent negative values.\n",
    "\n",
    "\n",
    "We run simulations of 8 scenarios that vary across the following conditions:\n",
    "amount of data truncation, volume of demand, and variability of cancel rate. We\n",
    "test two parameter options for each of these dimensions of variation. We vary data\n",
    "truncation by setting our benchmark Retail-1-threshold policies to single thresholds\n",
    "of either 8 or 14, allowing approximately 1/3 and 2/3 of total demand to get truncated,\n",
    "respectively. Volume of demand is varied by allowing either 40 or 400 time units\n",
    "for the demand generating Poisson process during each period of the simulation.\n",
    "We control cancel variability by running simulations where the standard deviation\n",
    "of cancel parameter b is .05 as well as .15. We test the Onera and Retail-1-threshold\n",
    "policies on 10 runs of each of the 8 (2/3) combinations of these simulation variants.\n",
    "We wish to compare against corresponding Onera policies. We use the corresponding\n",
    "Retail-1-threshold policy as the starting threshold for the Onera policies and then\n",
    "let the Onera policies train on the data it collects and determine its own thresholds\n",
    "for the remaining days. At the end of the simulation, we compare the Onera policy\n",
    "to the Retail-1-threshold policy in terms of cancel rate and revenues.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "global_seed = 0\n",
    "random.seed(global_seed)\n",
    "npr.seed(global_seed)\n",
    "\n",
    "class SafetyNetSimulation():\n",
    "    def __init__(self, \n",
    "                 is_high_demand=False, \n",
    "                 is_high_truncation=False, \n",
    "                 is_high_variable_cancel_rate=False,\n",
    "                 nCategories = 50, nThresholds = 20, \n",
    "                 n_demand_mean = 10, n_demand_stddev = 5,\n",
    "                 n_inventory_rate_min = 1, n_inventory_rate_max = 20, # inventory rate bounds\n",
    "                 category_price_mean = 800, category_price_stddev = 400, # price moments\n",
    "                 a_mean = .5, a_stddev = .1, b_mean = -.2): # cancel params\n",
    "        \n",
    "        self.is_high_demand = is_high_demand\n",
    "        self.is_high_truncation = is_high_truncation\n",
    "        self.is_high_variable_cancel_rate = is_high_variable_cancel_rate\n",
    "        \n",
    "        self.nCategories = nCategories\n",
    "        self.nThresholds = nThresholds\n",
    "         \n",
    "        \"\"\"\n",
    "        Each day of the simulation allows the Poisson processes generating \n",
    "        demand to run for either 40 or 400 time units (depending on simulation condition)\n",
    "        \"\"\"\n",
    "        if self.is_high_demand:\n",
    "            self.n_steps = 400\n",
    "        else:\n",
    "            self.n_steps = 40\n",
    "            \n",
    "        \"\"\"\n",
    "        We vary data truncation by setting our benchmark Retail-1-threshold policies to single thresholds\n",
    "        of either 8 or 14, allowing approximately 1/3 and 2/3 of total demand to get truncated,\n",
    "        respectively.\n",
    "        \"\"\"\n",
    "        if self.is_high_truncation:\n",
    "            self.retail_1_threshold_policy = 14\n",
    "        else:\n",
    "            self.retail_1_threshold_policy = 8\n",
    "        \n",
    "        \"\"\"\n",
    "        We control cancel variability by running simulations where the standard deviation\n",
    "        of cancel parameter b is .05 as well as .15.\n",
    "        \"\"\"\n",
    "        if self.is_high_variable_cancel_rate:\n",
    "            self.b_stddev = .15\n",
    "        else:\n",
    "            self.b_stddev = .05\n",
    "            \n",
    "\n",
    "        # Lambdas for Poisson Process generating Demand\n",
    "        self.category_demand_rate = npr.normal(loc=n_demand_mean, \n",
    "                                               scale=n_demand_stddev, \n",
    "                                               size=self.nCategories)\n",
    "        \n",
    "        self.category_demand_rate = np.clip(self.category_demand_rate, a_min=0, a_max=None)\n",
    "\n",
    "        # AVG PRICE PER CATEGORY\n",
    "        self.category_price_mean = category_price_mean\n",
    "        self.category_price_stddev = category_price_stddev\n",
    "\n",
    "        # INVENTORY LEVEL\n",
    "        self.category_inventory_level_rate = npr.uniform(high=n_inventory_rate_max, \n",
    "                                                    low=n_inventory_rate_min, \n",
    "                                                    size=n_category_clusters)\n",
    "        \n",
    "        self.category_inventory_level_rate = np.clip(self.category_inventory_level_rate, \n",
    "                                                     a_min=0, a_max=None)\n",
    "\n",
    "        # CANCELLATION PROBABILITY\n",
    "        self.a_param = npr.normal(loc=a_mean, scale=a_stddev, size=n_category_clusters)\n",
    "        self.b_param = npr.normal(loc=b_mean, scale=b_stddev, size=n_category_clusters)\n",
    "        \n",
    "    def category_demand_to_orders(self, category_demand):\n",
    "        \"\"\"\n",
    "        1 hot encode a category demand (nCategories,) as a matrix with dim (nOrders, nCategories)\n",
    "        Returned matrix tells us the category for each order\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        lc = len(category_demand)\n",
    "        for i, a in enumerate(category_demand):\n",
    "            for _ in range(a):\n",
    "                z = np.zeros(lc)\n",
    "                z[i] = 1\n",
    "                rows.append(z)\n",
    "        if len(rows) > 0:\n",
    "            return np.stack(rows)\n",
    "        else:\n",
    "            return np.array(rows)\n",
    "\n",
    "    def inv_count_from_orders(self, orders):\n",
    "        \"\"\"\n",
    "        For each order in (nOrders, nCategories), generates an inventory level using a poisson process \n",
    "        and 1-hot encodes the inventory at the time of the order in a (nOrders, nThresholds) matrix\n",
    "        If inventory exceeds nThresholds, cap it at nThreshold \n",
    "        [Not sure if this is the proper way to handle it]\n",
    "        \"\"\"\n",
    "        inv_count = []\n",
    "        for o in orders:\n",
    "            category_ind = np.argmax(o)\n",
    "            inv_count_row = np.zeros(shape=(self.nThresholds,))\n",
    "            inv_level = npr.poisson(self.category_inventory_level_rate[category_ind])\n",
    "            if inv_level >= self.nThresholds:\n",
    "                inv_level = self.nThresholds-1\n",
    "            inv_count_row[inv_level] = 1\n",
    "            inv_count.append(inv_count_row)\n",
    "\n",
    "        return np.array(inv_count)\n",
    "\n",
    "    def price_from_orders(self, orders):\n",
    "        \"\"\"\n",
    "        For each order, generate a price. \n",
    "        Returns a (nOrders,) vector\n",
    "        \"\"\"\n",
    "        return npr.normal(loc=self.category_price_mean, \n",
    "                          scale=self.category_price_stddev, \n",
    "                          size=len(orders))\n",
    "\n",
    "    def cancel_prob(self, orders, inv_count):\n",
    "        \"\"\"\n",
    "        For each order, calculate a cancellation probability according to that categories params\n",
    "        returns a (nOrders,) vector\n",
    "        \"\"\"\n",
    "        order_cat = np.argmax(orders, axis= 1) # get category for each order\n",
    "        inv_cat = np.argmax(inv_count, axis=1) # get inventory for each order\n",
    "        a_param_all = [self.a_param[i] for i in order_cat]\n",
    "        b_param_all = [self.b_param[i] for i in order_cat]\n",
    "        cancel_probs = 1.0 - 1.0/(1.0+ np.exp(a_param_all + (b_param_all * inv_cat)))\n",
    "        return cancel_probs\n",
    "    \n",
    "    def generate_data(self):\n",
    "        \n",
    "        X_T = [np.random.poisson(np.clip(rate, a_min=0, a_max=None) , size=self.n_steps) \n",
    "               for rate in self.category_demand_rate]\n",
    "        self.category_demand = np.array([np.sum(X) for X in X_T])\n",
    "             \n",
    "        orders = self.category_demand_to_orders(category_demand)\n",
    "        np.random.shuffle(orders)\n",
    "\n",
    "        inv_count = self.inv_count_from_orders(orders)\n",
    "        price = self.price_from_orders(orders)\n",
    "        cancel_probs = self.cancel_prob(orders, inv_count)\n",
    "\n",
    "        collection_thresholds = np.zeros(shape=(orders.shape[0],self.nThresholds))\n",
    "        # do we subtract 1 or not?\n",
    "        collection_thresholds[:,self.retail_1_threshold_policy-1]=1\n",
    "        \n",
    "        # Some sanity checks\n",
    "        assert orders.shape == (orders.shape[0], self.nCategories)\n",
    "        assert inv_count.shape == (orders.shape[0], self.nThresholds)\n",
    "        assert price.shape == (orders.shape[0],)\n",
    "        assert cancel_probs.shape == (orders.shape[0],)\n",
    "        assert collection_thresholds.shape == (orders.shape[0],self.nThresholds)\n",
    "        \n",
    "        \n",
    "        return orders, inv_count, price, cancel_probs, collection_thresholds\n",
    "    \n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_net_simulation = SafetyNetSimulation(\n",
    "                 is_high_demand=False, \n",
    "                 is_high_truncation=False, \n",
    "                 is_high_variable_cancel_rate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders, inv_count, price, cancel_probs, collection_thresholds = safety_net_simulation.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Pass, turn off LP layer\n",
    "safety_net = SafetyNet(\n",
    "    nKnapsackCategories=safety_net_simulation.nCategories, \n",
    "    nThresholds=safety_net_simulation.nThresholds, \n",
    "    starting_thresholds=torch.ones(safety_net_simulation.nCategories, safety_net_simulation.nThresholds), # all possible thresholds\n",
    "    parametric_knapsack=False # end to end safety net if true, no optimization lp layer if False \n",
    "    #(net will directly update the weights)\n",
    ")\n",
    "\n",
    "(new_revenue_loss, \n",
    " new_cancel_constraint_loss, \n",
    " new_accept_constraint_loss, \n",
    " arrival_probability_batch_by_threshold, \n",
    " log_arrival_prob, \n",
    " log_cancel_prob, \n",
    " log_category_prob, \n",
    " estimated_batch_total_demand, \n",
    " observed_cancel_constraint_loss, \n",
    " observed_accept_constraint_loss, \n",
    " lp_infeasible) = safety_net.forward(\n",
    "    category=Variable(torch.from_numpy(orders.astype(np.float32))),\n",
    "    inv_count=torch.from_numpy(inv_count.astype(np.float32)),\n",
    "    price=torch.from_numpy(price.astype(np.float32)),\n",
    "    cancel=torch.from_numpy(cancel_probs.astype(np.float32)),\n",
    "    collection_thresholds=torch.from_numpy(collection_thresholds.astype(np.float32))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# category clusters\n",
    "n_category_clusters = 50\n",
    "\n",
    "# distribution of rate parameter for poisson process that generates demand for each cluster\n",
    "n_demand_mean = 10\n",
    "n_demand_stddev = 5\n",
    "\n",
    "# Number of steps to run the poisson process for\n",
    "n_steps = 40 # or 400\n",
    "\n",
    "# rate paramter for possion distribution that models inventory for each category\n",
    "n_inventory_rate_min = 1\n",
    "n_inventory_rate_max = 20\n",
    "\n",
    "# moments for normal distribution characterizing price\n",
    "category_price_mean = 800\n",
    "category_price_stddev = 400\n",
    "\n",
    "# Simulations params for cancellation\n",
    "a_mean = .5\n",
    "a_stddev = .1\n",
    "\n",
    "b_mean = -.2\n",
    "b_stddev = .05\n",
    "\n",
    "# DEMAND \n",
    "category_demand_rate = npr.normal(loc=n_demand_mean, scale=n_demand_stddev, size=n_category_clusters)\n",
    "X_T = [np.random.poisson(np.clip(rate, a_min=0, a_max=None) , size=n_steps) for rate in category_demand_rate]\n",
    "category_demand = np.array([np.sum(X) for X in X_T])\n",
    "\n",
    "# AVG PRICE PER CATEGORY\n",
    "category_price = npr.normal(loc=category_price_mean, scale=category_price_stddev, size=n_category_clusters)\n",
    "\n",
    "# INVENTORY LEVEL\n",
    "category_inventory_level_rate = npr.uniform(high=n_inventory_rate_max, \n",
    "                                            low=n_inventory_rate_min, \n",
    "                                            size=n_category_clusters)\n",
    "category_inventory_level = npr.poisson(lam=category_inventory_level_rate)\n",
    "\n",
    "# CANCELLATION PROBABILITY\n",
    "a_param = npr.normal(loc=a_mean, scale=a_stddev, size=n_category_clusters)\n",
    "b_param = npr.normal(loc=b_mean, scale=b_stddev, size=n_category_clusters)\n",
    "\n",
    "category_cancel_prob = 1.0 - 1.0/(1.0+ np.exp(a_param + (b_param * category_inventory_level)))\n",
    "\n",
    "# Store and Clip\n",
    "params = {\n",
    "    'inventory': category_inventory_level,\n",
    "    'demand': category_demand,\n",
    "    'price': category_price,\n",
    "    'cancellation_probability': category_cancel_prob\n",
    "}\n",
    "\n",
    "for k,v in params.items():\n",
    "    params[k] = np.clip(v, a_min=0, a_max=None)\n",
    "\n",
    "    \n",
    "# safety net params\n",
    "nCategories = 50\n",
    "nThresholds = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_demand_to_orders(category_demand):\n",
    "    rows = []\n",
    "    lc = len(category_demand)\n",
    "    for i, a in enumerate(category_demand):\n",
    "        for _ in range(a):\n",
    "            z = np.zeros(lc)\n",
    "            z[i] = 1\n",
    "            rows.append(z)\n",
    "    if len(rows) > 0:\n",
    "        return np.stack(rows)\n",
    "    else:\n",
    "        return np.array(rows)\n",
    "\n",
    "def inv_count_from_orders(category_inventory_level_rate, orders):\n",
    "    inventory = []\n",
    "    for o in orders:\n",
    "        category_ind = np.argmax(o)\n",
    "        inv_count_row = np.zeros(shape=(nThresholds,))\n",
    "        inv_level = npr.poisson(category_inventory_level_rate[category_ind])\n",
    "        if inv_level >= nThresholds:\n",
    "            inv_level = nThresholds-1\n",
    "        inv_count_row[inv_level] = 1\n",
    "        inventory.append(inv_count_row)\n",
    "    \n",
    "    return np.array(inventory)\n",
    "\n",
    "def price_from_orders(category_price_mean,category_price_stddev, orders):\n",
    "    return npr.normal(loc=category_price_mean, scale=category_price_stddev, size=len(orders))\n",
    "\n",
    "def cancel_prob(a_param, b_param, orders, inv_count):\n",
    "    order_cat = np.argmax(orders, axis= 1)\n",
    "    inv_cat = np.argmax(inv_count, axis=1)\n",
    "    a_param_all = [a_param[i] for i in order_cat]\n",
    "    b_param_all = [b_param[i] for i in order_cat]\n",
    "    cancel_probs = 1.0 - 1.0/(1.0+ np.exp(a_param_all + (b_param_all * inv_cat)))\n",
    "    return cancel_probs\n",
    "    \n",
    "\n",
    "orders = category_demand_to_orders(category_demand)\n",
    "np.random.shuffle(orders)\n",
    "\n",
    "inv_count = inv_count_from_orders(category_inventory_level_rate, orders)\n",
    "price = price_from_orders(category_price_mean, category_price_stddev, orders)\n",
    "cancel_probs = cancel_prob(a_param, b_param, orders, inv_count)\n",
    "\n",
    "start_thresh = 8\n",
    "collection_thresholds = np.zeros(shape=(orders.shape[0],nThresholds))\n",
    "collection_thresholds[:,start_thresh-1]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = SafetyNet(\n",
    "    nKnapsackCategories=nCategories, \n",
    "    nThresholds=nThresholds, \n",
    "    starting_thresholds=torch.ones(nCategories, nThresholds), # all possible thresholds\n",
    "    parametric_knapsack=False # end to end safety net if true, no optimization lp layer if False \n",
    "    #(net will directly update the weights)\n",
    ")\n",
    "\n",
    "(new_revenue_loss, \n",
    " new_cancel_constraint_loss, \n",
    " new_accept_constraint_loss, \n",
    " arrival_probability_batch_by_threshold, \n",
    " log_arrival_prob, \n",
    " log_cancel_prob, \n",
    " log_category_prob, \n",
    " estimated_batch_total_demand, \n",
    " observed_cancel_constraint_loss, \n",
    " observed_accept_constraint_loss, \n",
    " lp_infeasible) = temp_model.forward(\n",
    "    category=Variable(torch.from_numpy(orders.astype(np.float32))),\n",
    "    inv_count=torch.from_numpy(inv_count.astype(np.float32)),\n",
    "    price=torch.from_numpy(price.astype(np.float32)),\n",
    "    cancel=torch.from_numpy(cancel_probs.astype(np.float32)),\n",
    "    collection_thresholds=torch.from_numpy(collection_thresholds.astype(np.float32))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1289.6364)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_objective_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split our orders into batches, forward/backward for each batch of period 1 data\n",
    "- difference between the *new_* and *observered_*\n",
    "- where does likelihood measures fit into return values from forward pass\n",
    "- define the logic for updates -> but how are we measuring if a violation has occured?\n",
    "- what are the init parameters?\n",
    "- how do we update the target cancellation rate based on period 1 cancellation rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1289.6364),\n",
       " tensor(43568.9648),\n",
       " tensor(2476.2925),\n",
       " tensor([[ 1.4858e-08,  4.4574e-08,  6.6861e-08,  ...,  8.3788e-07,\n",
       "           3.8834e-07,  3.1263e-07],\n",
       "         [ 1.4858e-08,  4.4574e-08,  6.6861e-08,  ...,  8.3788e-07,\n",
       "           3.8834e-07,  3.1263e-07],\n",
       "         [ 1.4858e-08,  4.4574e-08,  6.6861e-08,  ...,  8.3788e-07,\n",
       "           3.8834e-07,  3.1263e-07],\n",
       "         ...,\n",
       "         [ 1.4858e-08,  4.4574e-08,  6.6861e-08,  ...,  8.3788e-07,\n",
       "           3.8834e-07,  3.1263e-07],\n",
       "         [ 1.4858e-08,  4.4574e-08,  6.6861e-08,  ...,  8.3788e-07,\n",
       "           3.8834e-07,  3.1263e-07],\n",
       "         [ 1.4858e-08,  4.4574e-08,  6.6861e-08,  ...,  8.3788e-07,\n",
       "           3.8834e-07,  3.1263e-07]]),\n",
       " tensor([[-17.5101, -16.7237, -16.3813,  ..., -13.9805, -14.7360,\n",
       "          -14.9468],\n",
       "         [-17.5101, -16.7237, -16.3813,  ..., -13.9805, -14.7360,\n",
       "          -14.9468],\n",
       "         [-17.5101, -16.7237, -16.3813,  ..., -13.9805, -14.7360,\n",
       "          -14.9468],\n",
       "         ...,\n",
       "         [-17.5101, -16.7237, -16.3813,  ..., -13.9805, -14.7360,\n",
       "          -14.9468],\n",
       "         [-17.5101, -16.7237, -16.3813,  ..., -13.9805, -14.7360,\n",
       "          -14.9468],\n",
       "         [-17.5101, -16.7237, -16.3813,  ..., -13.9805, -14.7360,\n",
       "          -14.9468]]),\n",
       " tensor([[-0.0298, -3.5297],\n",
       "         [-0.1394, -2.0394],\n",
       "         [-0.3412, -1.2412],\n",
       "         ...,\n",
       "         [-0.6444, -0.7444],\n",
       "         [-0.3412, -1.2412],\n",
       "         [-0.3412, -1.2412]]),\n",
       " tensor([[-3.9120, -3.9120, -3.9120,  ..., -3.9120, -3.9120, -3.9120],\n",
       "         [-3.9120, -3.9120, -3.9120,  ..., -3.9120, -3.9120, -3.9120],\n",
       "         [-3.9120, -3.9120, -3.9120,  ..., -3.9120, -3.9120, -3.9120],\n",
       "         ...,\n",
       "         [-3.9120, -3.9120, -3.9120,  ..., -3.9120, -3.9120, -3.9120],\n",
       "         [-3.9120, -3.9120, -3.9120,  ..., -3.9120, -3.9120, -3.9120],\n",
       "         [-3.9120, -3.9120, -3.9120,  ..., -3.9120, -3.9120, -3.9120]]),\n",
       " tensor(1.00000e+05 *\n",
       "        6.4219),\n",
       " tensor(1176.8208),\n",
       " tensor(-171.7754),\n",
       " 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training after 1st pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model.parametric_knapsack = True\n",
    "\n",
    "temp_model.forward(\n",
    "    category=Variable(torch.from_numpy(orders.astype(np.float32))),\n",
    "    inv_count=torch.from_numpy(inv_count.astype(np.float32)),\n",
    "    price=torch.from_numpy(price.astype(np.float32)),\n",
    "    cancel=torch.from_numpy(cancel_probs.astype(np.float32)),\n",
    "    collection_thresholds=torch.from_numpy(collection_thresholds.astype(np.float32))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Safety Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = SafetyNet(\n",
    "    nKnapsackCategories=nCategories, \n",
    "    nThresholds=nThresholds, \n",
    "    starting_thresholds=torch.ones(nCategories, nThresholds), # all possible thresholds\n",
    "    parametric_knapsack=True # end to end safety net if true, no optimization lp layer if False \n",
    "    #(net will directly update the weights)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize Optimization/Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JK provided learning rates\n",
    "temp_optimizer_est = torch.optim.SGD([\n",
    "{'params':[temp_model.prices_est]}, #what will happen here since there is no learning rate? will it always stay the same?\n",
    "{'params':[temp_model.inventory_lam_est], 'lr': 1e-0},\n",
    "{'params':[temp_model.demand_distribution_est], 'lr': 1e-2},\n",
    "{'params':[temp_model.cancel_coef_est], 'lr': 5e-3},\n",
    "{'params':[temp_model.cancel_intercept_est], 'lr': 1e-1}\n",
    "    ], lr=1e-7)\n",
    "\n",
    "temp_optimizer_opt = torch.optim.SGD([\n",
    "{'params':[temp_model.inventory_lam_opt], 'lr': 2e-5},\n",
    "{'params':[temp_model.demand_distribution_opt], 'lr': 1e-5},\n",
    "{'params':[temp_model.cancel_coef_opt], 'lr': 2e-7},\n",
    "{'params':[temp_model.cancel_intercept_opt], 'lr': 2e-6}\n",
    "    ], lr=1e-8)\n",
    "\n",
    "temp_optimizer_RHS =torch.optim.SGD([\n",
    "{'params':[temp_model.cancel_rate_param, temp_model.accept_rate_param], 'lr': 1e-6}\n",
    "    ], lr=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Period 1 training\n",
    "\n",
    "# zero grad\n",
    "temp_optimizer_est.zero_grad()\n",
    "temp_optimizer_opt.zero_grad()\n",
    "temp_optimizer_RHS.zero_grad()\n",
    "\n",
    "# backward passes\n",
    "if is_cancel_constraint_violated:\n",
    "    new_cancel_constraint_loss.backward()\n",
    "    temp_optimizer_RHS.step()\n",
    "elif is_other_constraints_violated:\n",
    "    new_accept_constraint_loss.backward()\n",
    "    temp_optimizer_RHS.step()\n",
    "else:\n",
    "    new_revenue_loss.backward()\n",
    "    temp_optimizer_est.step()\n",
    "    # manually copy values of temp_optimizer_est into temp_optimizer_opt params \n",
    "\n",
    "## From period 2 onwards\n",
    "\n",
    "# zero grad\n",
    "temp_optimizer_est.zero_grad()\n",
    "temp_optimizer_opt.zero_grad()\n",
    "temp_optimizer_RHS.zero_grad()\n",
    "\n",
    "# backward\n",
    "if is_cancel_constraint_violated:\n",
    "    new_cancel_constraint_loss.backward()\n",
    "    temp_optimizer_RHS.step()\n",
    "elif is_other_constraints_violated:\n",
    "    new_accept_constraint_loss.backward()\n",
    "    temp_optimizer_RHS.step()\n",
    "else:\n",
    "    new_revenue_loss.backward()\n",
    "    temp_optimizer_opt.step()\n",
    "    temp_optimizer_est.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass \n",
    "\n",
    "Number of orders == Batch Size or nBatch\n",
    "\n",
    "Category: what is the category for each order -> (nBatch x nCategory)\n",
    "\n",
    "Inv Count: what is the inventory count for each order? nBatch x 1 (makes sense). nBatch x nThresholds works. ??\n",
    "- we may be using a cracked version from changes we made. try to rerun with a clean version of KnapsackNets.py\n",
    "\n",
    "Price: what is the price for each order? nBatch x 1\n",
    "\n",
    "Cancel: What is the cancel status (probability or binary?) for each order? \n",
    "\n",
    "Collection Thresholds: ??? dimensions indicate all possible thresholds for all orders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nBatch = 32\n",
    "temp_model.forward(\n",
    "    category=Variable(torch.from_numpy(category_demand.reshape(1,nCategories).astype(np.float32)).expand(nBatch,nCategories)),\n",
    "    inv_count=torch.from_numpy(category_inventory_level[:nThresholds].reshape(1,nThresholds).astype(np.float32)).expand(nBatch,nThresholds),\n",
    "    price=torch.from_numpy(category_price[:nBatch].reshape(nBatch).astype(np.float32)).expand(nBatch),\n",
    "    cancel=torch.from_numpy(category_cancel_prob[:nBatch].reshape(nBatch).astype(np.float32)).expand(nBatch),\n",
    "    collection_thresholds=torch.ones(nBatch,nThresholds)*10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Poisson Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PoissonFunction(Function):\n",
    "    def __init__(self,nKnapsackCategories,nThresholds, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.nKnapsackCategories = nKnapsackCategories\n",
    "        self.nThresholds = nThresholds\n",
    "    def forward(self, lam, thresholds):\n",
    "        self.poisson_dist = None\n",
    "        self.lam_matrix = torch.ger(lam,torch.ones(self.nThresholds)) # C X T -> 3's\n",
    "        self.t_matrix = torch.ger(torch.ones(self.nKnapsackCategories),thresholds) # C x T -> 0's\n",
    "        self.lam_pow_t_div_t_factorial = LamToverTFactorial(self.nKnapsackCategories,self.nThresholds,thresholds,lam)\n",
    "        #lam_pow_t_matrix = torch.pow(self.lam_matrix,self.t_matrix)\n",
    "        self.exp_lam_matrix = torch.exp(-1*self.lam_matrix)\n",
    "        #t_factorial = FactorialFunctionPlain(self.nThresholds,thresholds)\n",
    "        #self.t_factorial_matrix = torch.ger(torch.ones(self.nKnapsackCategories),t_factorial)\n",
    "        #print(\"PoissonFunction t_factorial_matrix\",self.t_factorial_matrix)\n",
    "        self.poisson_dist = self.lam_pow_t_div_t_factorial*self.exp_lam_matrix\n",
    "        #print(\"PoissonFunction lam_pow_t_div_t_factorial\",self.lam_pow_t_div_t_factorial)\n",
    "        #print(\"PoissonFunction exp_lam_matrix\",self.exp_lam_matrix)\n",
    "        #print(\"PoissonFunction poisson_dist\",self.poisson_dist)\n",
    "        self.save_for_backward(lam)\n",
    "        return self.poisson_dist\n",
    "    def backward(self, grad_output):\n",
    "        #print(\"PoissonFunction grad_output\", grad_output)\n",
    "        grad_lam = grad_thresholds = None\n",
    "        #lam_t_minus_one_matrix = torch.pow(self.lam_matrix,self.t_matrix-1)\n",
    "        lam_pow_t_minus_one_div_t_factorial = torch.div(self.lam_pow_t_div_t_factorial,self.lam_matrix)\n",
    "        t_minus_lam_matrix = self.t_matrix-self.lam_matrix\n",
    "        #print(\"PoissonFunction exp_lam_matrix\",self.exp_lam_matrix)\n",
    "        #print(\"PoissonFunction lam_pow_t_minus_one_div_t_factorial\", lam_pow_t_minus_one_div_t_factorial)\n",
    "        dp_dlam = self.exp_lam_matrix*lam_pow_t_minus_one_div_t_factorial*t_minus_lam_matrix\n",
    "        #print(\"PoissonFunction dp_dlam\", dp_dlam)\n",
    "        grad_lam = torch.sum(grad_output*dp_dlam,dim=1).squeeze()\n",
    "        #print(\"PoissonFunction grad_lam\", grad_lam)\n",
    "        return grad_lam, grad_thresholds\n",
    "\n",
    "def LamToverTFactorial(nKnapsackCategories,nThresholds,t_vector,lam_vector):\n",
    "    soln_matrix = torch.ones(nKnapsackCategories,nThresholds)\n",
    "    lam_matrix = torch.ger(lam_vector, torch.ones(nThresholds))\n",
    "    onesCategories = torch.ones(nKnapsackCategories)\n",
    "    t_matrix = torch.ger(torch.ones(nKnapsackCategories),t_vector)\n",
    "    for i in range(nThresholds):\n",
    "        lam_matrix = lam_matrix.t()\n",
    "        lam_matrix[i]=onesCategories\n",
    "        lam_matrix = lam_matrix.t()\n",
    "        soln_matrix = torch.div(soln_matrix*lam_matrix,(t_matrix-i).clamp(min=1))\n",
    "    return soln_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pf = PoissonFunction(nCategories,nThresholds,verbose=-1)\n",
    "eps=1e-8\n",
    "thresholds = Variable(torch.arange(0,nThresholds))\n",
    "inventory_initializer = 3\n",
    "inventory_lam_est = Parameter(torch.ones(nCategories)*inventory_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_distribution_raw_est = pf(inventory_lam_est,thresholds)+ eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_distribution_raw_est.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancel Rate by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_rate_belief_cXt(cancel_coefs,cancel_intercepts,thresholds_matrix):\n",
    "    #1-(1/(1+math.exp(cancel_intercept+x*cancel_coef)\n",
    "    cancel_intercepts_matrix = cancel_intercepts.unsqueeze(1).expand_as(thresholds_matrix)\n",
    "    cancel_coefs_matrix = cancel_coefs.unsqueeze(1).expand_as(thresholds_matrix)\n",
    "    intercept_plus_thresh_times_coef = cancel_intercepts_matrix+(thresholds_matrix*cancel_coefs_matrix)\n",
    "    exp_matrix = torch.exp(intercept_plus_thresh_times_coef)\n",
    "    cancel_rate_belief = 1-(1/(1+exp_matrix))\n",
    "    return cancel_rate_belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "self.belief_cancel_rate_cXt_est = cancel_rate_belief_cXt(self.cancel_coef_neg_est,\n",
    "                                                         self.cancel_intercept_est,\n",
    "                                                         self.thresholds.unsqueeze(0).expand(self.nKnapsackCategories,self.nThresholds))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.ones(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
